{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Advanced-StyleGAN_Network-bending.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j3nsykes/CoLabNotebooks/blob/main/Advanced_StyleGAN_Network_bending.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRGVsNV4APak"
      },
      "source": [
        "# Network Bending\n",
        "## Manipulate StyleGAN2 models through rotation, translation, etc.\n",
        "\n",
        "[Paper](https://arxiv.org/abs/2005.12420) | [Video](https://youtu.be/IlSMQ2RRTh8) | [GitHub](https://github.com/terrybroad/network-bending)\n",
        "\n",
        "Thanks to [Sid Black](https://twitter.com/realmeatyhuman) for a lot of the code used here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sTjMUOhAdVH"
      },
      "source": [
        "## Install Library\n",
        "\n",
        "This code uses the Rosinality version of StyleGAN2. Because of that the install process for this may take a couple minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjMpaW7W5uYA"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq4gcOTXwzSV"
      },
      "source": [
        "# Install libraries\n",
        "!git clone -b audio-animate https://github.com/dvschultz/network-bending\n",
        "!pip uninstall torch torchvision -y\n",
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install Ninja kmeans-pytorch\n",
        "!apt-get install -y vim make gdb libopencv-dev\n",
        "!wget https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.5.0%2Bcu101.zip\n",
        "!unzip /content/libtorch-shared-with-deps-1.5.0+cu101.zip -d /root/\n",
        "%cd network-bending\n",
        "\n",
        "#build custom pytorch transformations\n",
        "!chmod +x /content/network-bending/build_custom_transforms.sh\n",
        "!/content/network-bending/build_custom_transforms.sh /root/libtorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N7SVhtI-Ep0"
      },
      "source": [
        "def show_local_mp4_video(file_name, width=640, height=640):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlKlP0NoB8Zp"
      },
      "source": [
        "## Download .pt file\n",
        "\n",
        "As mentioned above, this library uses the Rosinality version of StyleGAN2. If you have a .pkl file, youâ€™ll need to convert it to a .pt file. I have a notebook to do that [here](https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PT_to_Rosinality.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ijf6j9Iw8_u"
      },
      "source": [
        "!gdown --id 1rL-J63eFfn80IYU2GfVY977GI2qOG6dw -O /content/ladiesblack.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJ33m8GCdW5"
      },
      "source": [
        "## Generate Image Samples (and Latent Vectors)\n",
        "\n",
        "This script will generate \"normal\" images because the transform config is blank. I recommend doing this initially so you know what images you want to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPlwCpEBd5VT"
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK8tkjZ5OqlW"
      },
      "source": [
        "!python generate.py \\\n",
        "--ckpt /content/ladiesblack.pt \\\n",
        "--pics 20 \\\n",
        "--config /content/network-bending/configs/empty_transform_config.yaml \\\n",
        "--save_latent 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bazmXRa0zhw"
      },
      "source": [
        "!zip -r samples-ladiesblack-400.zip ./sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8mSHIfqAwZS"
      },
      "source": [
        "If we want to generate images using transformations we have to create a config file, update its values and then run the `generate.py` script using the same config.\n",
        "\n",
        "Note: order of transforms does matter!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roIn2TPzA79j"
      },
      "source": [
        "!cp ./configs/empty_transform_config.yaml ./configs/custom_transform_config.yaml "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-VX0EqVBIhd"
      },
      "source": [
        "%%writefile ./configs/custom_transform_config.yaml \n",
        "transforms:\n",
        "- layer: 1\n",
        "  transform: \"rotate\"\n",
        "  params: [45.0]\n",
        "  features: \"all\"\n",
        "- layer: 10\n",
        "  transform: \"rotate\"\n",
        "  params: [45.0]\n",
        "  features: \"all\"\n",
        "- layer: 3\n",
        "  transform: \"translate\"\n",
        "  params: [-0.5, -0.25] #range is -1 to 1\n",
        "  features: \"all\"\n",
        "- layer: 8\n",
        "  transform: \"scale\"\n",
        "  params: [1.5]\n",
        "  features: \"all\"\n",
        "- layer: 15\n",
        "  transform: \"flip-h\"\n",
        "  params: []\n",
        "  features: \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wbkAQwzB9W6"
      },
      "source": [
        "!python generate.py --ckpt /content/ladiesblack.pt --pics 10 --config ./configs/custom_transform_config.yaml --dir '/content/custom-samples/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRIJcNbOAbHS"
      },
      "source": [
        "You can also generate strips of images where the transform is applied to every single layer insequence. Note this requires a separate transformation config file as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZZ5RaSZLxdG"
      },
      "source": [
        "!cp ./configs/sample_strip_config.yaml ./configs/custom_strip_config.yaml "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLmrvXDEMClO"
      },
      "source": [
        "%%writefile ./configs/custom_strip_config.yaml \n",
        "transform: \"rotate\"\n",
        "params: [45.0]\n",
        "features: \"all\"\n",
        "feature-param: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7LtrjsMyL-5"
      },
      "source": [
        "!python generate_sample_strips.py \\\n",
        "--ckpt /content/ladiesblack.pt \\\n",
        "--pics 5 \\\n",
        "--config ./configs/custom_strip_config.yaml   \\\n",
        "--dir '/content/strips/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeY-khx-edMt"
      },
      "source": [
        "## Animating Vectors: Script version\n",
        "\n",
        "You can use a script based version if you want to create interpolations with single transformations.\n",
        "\n",
        "*   `--num_frames`: how many frames to produce (this value/fps in video = length of animation)\n",
        "*   `--transform`: transform function you want to use\n",
        "*   `--init_val`, `--end_val`: starting and stoppping points for linear transformation over the total frames\n",
        "* `--layer_id`: which of the StyleGAN layers to apply the transformation to. Lower IDs will affect more of the structure, higher IDs will affect more of the details.\n",
        "* `--interpolate_ids`: which of the StyleGAN layers to \n",
        "\n",
        "apply the transformation to. Lower IDs will affect more of the structure, higher IDs will affect more of the details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJv_npMeYE8A"
      },
      "source": [
        "!python animate.py \\\n",
        "--ckpt /content/ladiesblack.pt \\\n",
        "--load_latent /content/network-bending/sample/latents.yaml \\\n",
        "--interpolate_ids=10,10 \\\n",
        "--latent_id 0 \\\n",
        "--num_frames 240 \\\n",
        "--transform \"scale\" \\\n",
        "--init_val 0.0 \\\n",
        "--end_val 3.0 \\\n",
        "--layer_id=1 \\\n",
        "--truncation=0.6 \\\n",
        "--noise_interpolation \\\n",
        "--dir=\"ladiesblack-scale-test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk6pQjc69pMJ"
      },
      "source": [
        "!rm -r /content/network-bending/ladiesblack-scale-test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-n8Vl8--J12"
      },
      "source": [
        "show_local_mp4_video('/content/network-bending/ladiescrop28-rotate-layer3.mp4', width=720, height=720)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghTBBw2pjjk4"
      },
      "source": [
        "### Multiple Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MOxD-XwjneF"
      },
      "source": [
        "!python animate.py \\\n",
        "--ckpt /content/ladiesblack.pt \\\n",
        "--load_latent /content/network-bending/sample/latents.yaml \\\n",
        "--latent_id 0 \\\n",
        "--num_frames 72 \\\n",
        "--transform \"scale,rotate,scale\" \\\n",
        "--init_val 0.0,0.0,0.5 \\\n",
        "--end_val 2.0,360.0,1.0 \\\n",
        "--layer_id 3,1,10 \\\n",
        "--truncation=0.5 \\\n",
        "--interpolate_ids=1,4,11,1 \\\n",
        "--dir=\"ladiesblack-multi-test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2eJT9dYP0Q6"
      },
      "source": [
        "### Interpolating Noise\n",
        "\n",
        "You may find that the detail textures on your bend animations seem too similar. You can add `--noise_interpolation` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLbwmTzPvTc"
      },
      "source": [
        "!python animate.py \\\n",
        "--ckpt /content/ladiesblack.pt \\\n",
        "--load_latent /content/network-bending/sample/latents.yaml \\\n",
        "--latent_id 0 \\\n",
        "--num_frames 72 \\\n",
        "--transform \"scale,rotate,scale\" \\\n",
        "--init_val 0.0,0.0,0.5 \\\n",
        "--end_val 2.0,360.0,1.0 \\\n",
        "--layer_id 3,1,10 \\\n",
        "--truncation=0.5 \\\n",
        "--interpolate_ids=1,4,11,1 \\\n",
        "--dir=\"ladiesblack-multi-test-noise2\" \\\n",
        "--noise_interpolation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXus8fcACj7V"
      },
      "source": [
        "## Generate Clusters\n",
        "\n",
        "More TK on this section because honestly Iâ€™m not sure this is all thatâ€™s needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-l9xvCyVeZ"
      },
      "source": [
        "!python get_clusters.py --ckpt /content/network-bending/FreaGAN-10k.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD2dDdMig3_8"
      },
      "source": [
        "##Handmade stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCfD-5bNzsld"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from torchvision import utils\n",
        "from model import Generator\n",
        "from tqdm import tqdm\n",
        "from util import *\n",
        "\n",
        "frames = 120\n",
        "\n",
        "# https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/4\n",
        "def slerp(val, low, high):\n",
        "    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm*high_norm).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high\n",
        "    print(res.shape)\n",
        "    return res\n",
        "\n",
        "device = \"cuda\"\n",
        "g_ema = Generator(\n",
        "        1024, 512, 8, 2\n",
        "    ).to(device)\n",
        "checkpoint = torch.load('/content/ladiesblack.pt')\n",
        "g_ema.load_state_dict(checkpoint[\"g_ema\"])\n",
        "\n",
        "with torch.no_grad():\n",
        "    mean_latent = g_ema.mean_latent(4096)\n",
        "\n",
        "yaml_config = {}\n",
        "with open('/content/network-bending/configs/empty_transform_config.yaml', 'r') as stream:\n",
        "    try:\n",
        "        yaml_config = yaml.load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "cluster_config = {}\n",
        "layer_channel_dims = create_layer_channel_dim_dict(2)\n",
        "\n",
        "# create noise\n",
        "noise = [getattr(g_ema.noises, f\"noise_{i}\") for i in range(g_ema.num_layers)]\n",
        "\n",
        "noise2 = copy.deepcopy(noise)\n",
        "for i,n in enumerate(noise2):\n",
        "    if len(n[0][0]) < 256:\n",
        "        # print('update: ', n.shape)\n",
        "        noise2[i] = (0.1**0.5)*torch.randn_like(n)\n",
        "\n",
        "# only slerp for lower layers, keep defaults for higher layers (won't fit in VRAM)\n",
        "# noise_slerps = []\n",
        "# for f in range(int(frames/2)):\n",
        "#     ns = []\n",
        "#     for i in range(len(noise)):\n",
        "#         if len(noise[i][0][0]) < 256:\n",
        "#             # print('update: ', noise[i].shape)\n",
        "#             ns.append(slerp(f/(frames/2), noise[i], noise2[i]))\n",
        "#     noise_slerps.append(ns)\n",
        "\n",
        "noise_slerps = []\n",
        "for f in range(int(frames/2)):\n",
        "    ns = []\n",
        "    for i in range(len(noise)):\n",
        "        # if len(noise[i][0][0]) < 256:\n",
        "            # print('update: ', noise[i].shape)\n",
        "        #ns.append(slerp((f/(frames/2)), noise[i], noise2[i]))\n",
        "        ns.append( torch.lerp( noise[i], noise2[i], (f/(frames/2)) ) )\n",
        "    noise_slerps.append(ns)\n",
        "\n",
        "#print(len(noise_slerps))\n",
        "\n",
        "with torch.no_grad():\n",
        "    g_ema.eval()\n",
        "    t_dict_list = create_transforms_dict_list(yaml_config, cluster_config, layer_channel_dims)\n",
        "    \n",
        "    sample_z = torch.randn(1, 512, device=device)\n",
        "    for i in tqdm(range(len(noise_slerps))):\n",
        "        \n",
        "        sample, _ = g_ema([sample_z], truncation=0.7, noise=noise_slerps[i], truncation_latent=mean_latent, transform_dict_list=t_dict_list)\n",
        "        #sample2, _ = g_ema([sample_z], truncation=0.7, noise=noise, truncation_latent=mean_latent, transform_dict_list=t_dict_list)\n",
        "\n",
        "        if not os.path.exists('interpolations'):\n",
        "            os.makedirs('interpolations')\n",
        "\n",
        "        utils.save_image(\n",
        "            sample,\n",
        "            f'interpolations/{str(i).zfill(6)}.png',\n",
        "            nrow=1,\n",
        "            normalize=True,\n",
        "            range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bGMzBm_g6_U"
      },
      "source": [
        "!ffmpeg -r 24 -i /content/network-bending/interpolations/%06d.png -vcodec libx264 -pix_fmt yuv420p noise-test.mp4 -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf6sZbOfeSVS"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYPiAAU6j_i1"
      },
      "source": [
        "rm -rf /content/network-bending/sample-animation/frame*.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ypfSHs27N6"
      },
      "source": [
        "!gdown --id 1rL-J63eFfn80IYU2GfVY977GI2qOG6dw -O /content/ladiesblack.pt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}